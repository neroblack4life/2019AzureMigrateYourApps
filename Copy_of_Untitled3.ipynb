{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neroblack4life/2019AzureMigrateYourApps/blob/master/Copy_of_Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVX8TbLq9BrW"
      },
      "outputs": [],
      "source": [
        "# llama.cpp Setup\n",
        "\n",
        "# 1. Install dependencies\n",
        "!apt-get update\n",
        "!apt-get install -y make g++ git cmake\n",
        "\n",
        "# 2. Clone llama.cpp repo\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "\n",
        "# 3. Build\n",
        "!make\n",
        "\n",
        "# 4. Download model weights into models folder\n",
        "# Instructions:\n",
        "# - Obtain original LLaMA model weights and place in ./models folder\n",
        "# - For BPE models, get vocab.json and add to models folder\n",
        "\n",
        "# 5. Convert model\n",
        "!python3 convert.py models/7B/\n",
        "\n",
        "# 6. Quantize model\n",
        "!./quantize ./models/7B/ggml-model-f16.bin ./models/7B/ggml-model-q4_0.bin q4_0\n",
        "\n",
        "# 7. Run inference\n",
        "print(\"Loading model...\")\n",
        "!./main -m ./models/7B/ggml-model-q4_0.bin -n 128 -p \"Hello world\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pyllama Setup\n",
        "\n",
        "# 1. Install pyllama\n",
        "!pip install pyllama\n",
        "\n",
        "# 2. Download model\n",
        "!python -m pyllama.download --model_size 7B\n",
        "\n",
        "# 3. Quantize model (optional)\n",
        "!python -m pyllama.llama_quant --model_size 7B --save pyllama-7B4b.pt\n",
        "\n",
        "# 4. Run inference\n",
        "import pyllama\n",
        "\n",
        "llama = pyllama.LLama(model_name=\"7B\", quantize=\"pyllama-7B4b.pt\")\n",
        "\n",
        "response = llama(\"Hello world\", use_gpu=False)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "2OsDfDfT9FfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install llama-api-server\n",
        "!pip install llama-api-server\n",
        "!pip install llama-api-server[pyllama]\n",
        "\n",
        "!cat > config.yml << EOF\n",
        "models:\n",
        "  completions:\n",
        "    text-ada-002:\n",
        "      type: llama_cpp\n",
        "      params:\n",
        "        path: /absolute/path/to/your/7B/ggml-model-q4_0.bin\n",
        "    text-davinci-002:\n",
        "      type: pyllama_quant\n",
        "      params:\n",
        "        path: /absolute/path/to/your/pyllama-7B4b.pt\n",
        "    text-davinci-003:\n",
        "      type: pyllama\n",
        "      params:\n",
        "        ckpt_dir: /absolute/path/to/your/7B/\n",
        "        tokenizer_path: /absolute/path/to/your/tokenizer.model\n",
        "      # keep to 1 instance to speed up loading of model\n",
        "  embeddings:\n",
        "    text-embedding-davinci-002:\n",
        "      type: pyllama_quant\n",
        "      params:\n",
        "        path: /absolute/path/to/your/pyllama-7B4b.pt\n",
        "      min_instance: 1\n",
        "      max_instance: 1\n",
        "      idle_timeout: 3600\n",
        "    text-embedding-ada-002:\n",
        "      type: llama_cpp\n",
        "      params:\n",
        "        path: /absolute/path/to/your/7B/ggml-model-q4_0.bin\n",
        "EOF\n",
        "\n",
        "!echo \"SOME_TOKEN\" > tokens.txt"
      ],
      "metadata": {
        "id": "xdv9hbYP9WY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call with openai-python\n",
        "\n",
        "import openai\n",
        "\n",
        "# Set API key and base URL\n",
        "openai.api_key = \"wWw2gocigV4a6wuUq4wNOy4wHPHz5VMw\"\n",
        "openai.api_base = \"http://127.0.0.1:5000/v1\"\n",
        "\n",
        "# Start the server\n",
        "!python -m llama_api_server &\n",
        "\n",
        "# Test completions\n",
        "response = openai.Completion.create(\n",
        "  engine=\"text-ada-002\",\n",
        "  prompt=\"Hello\",\n",
        "  max_tokens=5\n",
        ")\n",
        "\n",
        "print(response)\n",
        "\n",
        "# Test embeddings\n",
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"http://127.0.0.1:5000/v1/embeddings\"\n",
        "headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {openai.api_key}\"}\n",
        "data = json.dumps({\"model\": \"text-embedding-ada-002\", \"input\": \"Hello world\"})\n",
        "\n",
        "response = requests.post(url, headers=headers, data=data)\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "pYRxI6CJ9XO8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}